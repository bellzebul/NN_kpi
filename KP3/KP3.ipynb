{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "                                          МІНІСТЕРСТВО ОСВІТИ І НАУКИ УКРАЇНИ\n",
        "                                      НАЦІОНАЛЬНИЙ ТЕХНІЧНИЙ УНІВЕРСИТЕТ УКРАЇНИ\n",
        "                                          “КИЇВСЬКИЙ ПОЛІТЕХНІЧНИЙ ІНСТИТУТ”\n",
        "                                            Кафедра біомедичної кібернетики\n",
        "\n",
        "\n",
        "                                                  ПРАКТИЧНА РОБОТА 3\n",
        "\n",
        "                                            з дисципліни «Нейронні мережі»\n",
        "                                            «МЕРЕЖІ ДЛЯ ЗАДАЧІ КЛАСИФІКАЦІЇ,\n",
        "                                          ЗАСТОСОВУЮЧИ ГРАДІЄНТНИЙ СПУСК.»\n",
        "                                                        Варіант 3\n",
        "\n",
        "\n",
        "                                                                                Студента 3-го курсу\n",
        "                                                                                групи БС-11\n",
        "                                                                                Бутко В.Д.\n",
        "                                                      Київ 2023"
      ],
      "metadata": {
        "id": "5SNsNFbWTe9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "vpBSfgCBnOM3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "glass_identification = fetch_ucirepo(id=42)\n",
        "\n",
        "X = glass_identification.data.features\n",
        "Y = glass_identification.data.targets\n"
      ],
      "metadata": {
        "id": "ewwPofu2WMp7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y['Type_of_glass'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVtIYZd6Wkf3",
        "outputId": "5420d0c3-4191-46e9-fdba-6b1653ec612c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2    76\n",
              "1    70\n",
              "7    29\n",
              "3    17\n",
              "5    13\n",
              "6     9\n",
              "Name: Type_of_glass, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Бачу, що немає жодної мітки 4го класу. Це викличе проблемі потім, при побудові конфьюжин матриці, тому просто заміню назви міток 7го класу на 4й."
      ],
      "metadata": {
        "id": "XllaGg4gfdRF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Y['Type_of_glass'][Y['Type_of_glass'] == 7] = 4\n",
        "Y['Type_of_glass'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6wcl6JQf9kh",
        "outputId": "580056e3-721b-4467-93de-27eba3ad81f5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-817da7a67a64>:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  Y['Type_of_glass'][Y['Type_of_glass'] == 7] = 4\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2    76\n",
              "1    70\n",
              "4    29\n",
              "3    17\n",
              "5    13\n",
              "6     9\n",
              "Name: Type_of_glass, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler().fit(X)\n",
        "X = scaler.transform(X)"
      ],
      "metadata": {
        "id": "ihDE9TMWRTNk"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "89jyF5e2RjVp"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_one_hot(labels, num_classes=None):\n",
        "\n",
        "  if num_classes is None:\n",
        "    num_classes = np.max(labels) + 1\n",
        "  one_hot_array = np.zeros((len(labels), num_classes))\n",
        "  one_hot_array[np.arange(len(labels)), labels.squeeze()] = 1\n",
        "\n",
        "  return one_hot_array"
      ],
      "metadata": {
        "id": "VFHn3MfzSBwu"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_train_encoded = to_one_hot(Y_train.to_numpy())\n",
        "Y_test_encoded = to_one_hot(Y_test.to_numpy())"
      ],
      "metadata": {
        "id": "qWORPyEjSB1F"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)"
      ],
      "metadata": {
        "id": "6JSJb5PSyPP7"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "  return np.where(x > 0, 1, 0)"
      ],
      "metadata": {
        "id": "K_6m0qc7yRki"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def tanh_derivative(x):\n",
        "    return 1 - np.tanh(x)**2"
      ],
      "metadata": {
        "id": "ssnAL5uM0WMd"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    return e_x / np.sum(e_x, axis=1, keepdims=True)"
      ],
      "metadata": {
        "id": "lz79G0jv1_Y4"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_loss(Y, Y_hat):\n",
        "    m = Y.shape[0]\n",
        "    loss = -np.sum(Y * np.log(Y_hat)) / m\n",
        "    return loss"
      ],
      "metadata": {
        "id": "nL1NHCOl5PHS"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_parameters(input_size, hidden_size, num_classes):\n",
        "    params = {\n",
        "        'W1': np.random.randn(input_size, hidden_size) * 0.01,\n",
        "        'b1': np.zeros((1, hidden_size)),\n",
        "        'W2': np.random.randn(hidden_size, num_classes) * 0.01,\n",
        "        'b2': np.zeros((1, num_classes))\n",
        "    }\n",
        "    return params"
      ],
      "metadata": {
        "id": "PUpaLRygze3T"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_pass(X, params):\n",
        "    Z1 = np.dot(X, params['W1']) + params['b1']\n",
        "    A1 = sigmoid(Z1)\n",
        "    Z2 = np.dot(A1, params['W2']) + params['b2']\n",
        "    A2 = softmax(Z2)\n",
        "    forward_params = {'Z1': Z1, 'A1': A1, 'Z2': Z2, 'A2': A2}\n",
        "    return forward_params"
      ],
      "metadata": {
        "id": "BEgT8MaYzi2C"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def backward_pass(X, Y, params, forward_params, lr=0.01):\n",
        "    m = X.shape[0]\n",
        "    dZ2 = forward_params['A2'] - Y\n",
        "    dW2 = np.dot(forward_params['A1'].T, dZ2) / m\n",
        "    db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n",
        "    dA1 = np.dot(dZ2, params['W2'].T)\n",
        "    dZ1 = dA1 * sigmoid_derivative(forward_params['A1'])\n",
        "    dW1 = np.dot(X.T, dZ1) / m\n",
        "    db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n",
        "\n",
        "    params['W1'] -= lr * dW1\n",
        "    params['b1'] -= lr * db1\n",
        "    params['W2'] -= lr * dW2\n",
        "    params['b2'] -= lr * db2\n",
        "    return params\n"
      ],
      "metadata": {
        "id": "0_LGrnTS0VGc"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(X, Y, n_input, n_hidden, n_output, iters=100000, lr = 0.01):\n",
        "\n",
        "  params = initialize_parameters(n_input, n_hidden, n_output)\n",
        "  losses = []\n",
        "\n",
        "  for i in range(iters):\n",
        "\n",
        "    forward_params = forward_pass(X, params)\n",
        "\n",
        "    loss = compute_loss(Y, forward_params['A2'])\n",
        "    losses.append(loss)\n",
        "\n",
        "\n",
        "    grads = backward_pass(X, Y, params, forward_params, lr)\n",
        "\n",
        "    if i % 150 == 0:\n",
        "      print(f'Iteration {i}, Loss: {loss:.4f}')\n",
        "\n",
        "  return grads, losses"
      ],
      "metadata": {
        "id": "4zBIzGd105fq"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_input = 9\n",
        "n_hidden = 250\n",
        "n_output = 7"
      ],
      "metadata": {
        "id": "j19sVfXDV-9n"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params, losses = fit(X_train, Y_train_encoded, n_input, n_hidden, n_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKwCEsXWV2Al",
        "outputId": "64e9d16c-98f1-4f0d-bff9-3d0f0a3c1eca"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0, Loss: 1.9270\n",
            "Iteration 150, Loss: 1.4765\n",
            "Iteration 300, Loss: 1.4708\n",
            "Iteration 450, Loss: 1.4673\n",
            "Iteration 600, Loss: 1.4640\n",
            "Iteration 750, Loss: 1.4602\n",
            "Iteration 900, Loss: 1.4555\n",
            "Iteration 1050, Loss: 1.4494\n",
            "Iteration 1200, Loss: 1.4413\n",
            "Iteration 1350, Loss: 1.4306\n",
            "Iteration 1500, Loss: 1.4166\n",
            "Iteration 1650, Loss: 1.3986\n",
            "Iteration 1800, Loss: 1.3764\n",
            "Iteration 1950, Loss: 1.3501\n",
            "Iteration 2100, Loss: 1.3209\n",
            "Iteration 2250, Loss: 1.2907\n",
            "Iteration 2400, Loss: 1.2615\n",
            "Iteration 2550, Loss: 1.2349\n",
            "Iteration 2700, Loss: 1.2115\n",
            "Iteration 2850, Loss: 1.1913\n",
            "Iteration 3000, Loss: 1.1737\n",
            "Iteration 3150, Loss: 1.1583\n",
            "Iteration 3300, Loss: 1.1445\n",
            "Iteration 3450, Loss: 1.1320\n",
            "Iteration 3600, Loss: 1.1205\n",
            "Iteration 3750, Loss: 1.1097\n",
            "Iteration 3900, Loss: 1.0995\n",
            "Iteration 4050, Loss: 1.0898\n",
            "Iteration 4200, Loss: 1.0805\n",
            "Iteration 4350, Loss: 1.0716\n",
            "Iteration 4500, Loss: 1.0630\n",
            "Iteration 4650, Loss: 1.0548\n",
            "Iteration 4800, Loss: 1.0468\n",
            "Iteration 4950, Loss: 1.0391\n",
            "Iteration 5100, Loss: 1.0317\n",
            "Iteration 5250, Loss: 1.0246\n",
            "Iteration 5400, Loss: 1.0178\n",
            "Iteration 5550, Loss: 1.0113\n",
            "Iteration 5700, Loss: 1.0050\n",
            "Iteration 5850, Loss: 0.9990\n",
            "Iteration 6000, Loss: 0.9932\n",
            "Iteration 6150, Loss: 0.9878\n",
            "Iteration 6300, Loss: 0.9825\n",
            "Iteration 6450, Loss: 0.9775\n",
            "Iteration 6600, Loss: 0.9728\n",
            "Iteration 6750, Loss: 0.9683\n",
            "Iteration 6900, Loss: 0.9640\n",
            "Iteration 7050, Loss: 0.9599\n",
            "Iteration 7200, Loss: 0.9559\n",
            "Iteration 7350, Loss: 0.9522\n",
            "Iteration 7500, Loss: 0.9486\n",
            "Iteration 7650, Loss: 0.9452\n",
            "Iteration 7800, Loss: 0.9420\n",
            "Iteration 7950, Loss: 0.9389\n",
            "Iteration 8100, Loss: 0.9359\n",
            "Iteration 8250, Loss: 0.9330\n",
            "Iteration 8400, Loss: 0.9302\n",
            "Iteration 8550, Loss: 0.9276\n",
            "Iteration 8700, Loss: 0.9250\n",
            "Iteration 8850, Loss: 0.9226\n",
            "Iteration 9000, Loss: 0.9202\n",
            "Iteration 9150, Loss: 0.9178\n",
            "Iteration 9300, Loss: 0.9156\n",
            "Iteration 9450, Loss: 0.9134\n",
            "Iteration 9600, Loss: 0.9113\n",
            "Iteration 9750, Loss: 0.9092\n",
            "Iteration 9900, Loss: 0.9072\n",
            "Iteration 10050, Loss: 0.9052\n",
            "Iteration 10200, Loss: 0.9033\n",
            "Iteration 10350, Loss: 0.9014\n",
            "Iteration 10500, Loss: 0.8995\n",
            "Iteration 10650, Loss: 0.8977\n",
            "Iteration 10800, Loss: 0.8959\n",
            "Iteration 10950, Loss: 0.8941\n",
            "Iteration 11100, Loss: 0.8924\n",
            "Iteration 11250, Loss: 0.8907\n",
            "Iteration 11400, Loss: 0.8890\n",
            "Iteration 11550, Loss: 0.8873\n",
            "Iteration 11700, Loss: 0.8856\n",
            "Iteration 11850, Loss: 0.8840\n",
            "Iteration 12000, Loss: 0.8823\n",
            "Iteration 12150, Loss: 0.8807\n",
            "Iteration 12300, Loss: 0.8791\n",
            "Iteration 12450, Loss: 0.8775\n",
            "Iteration 12600, Loss: 0.8759\n",
            "Iteration 12750, Loss: 0.8743\n",
            "Iteration 12900, Loss: 0.8728\n",
            "Iteration 13050, Loss: 0.8712\n",
            "Iteration 13200, Loss: 0.8697\n",
            "Iteration 13350, Loss: 0.8681\n",
            "Iteration 13500, Loss: 0.8666\n",
            "Iteration 13650, Loss: 0.8651\n",
            "Iteration 13800, Loss: 0.8636\n",
            "Iteration 13950, Loss: 0.8621\n",
            "Iteration 14100, Loss: 0.8606\n",
            "Iteration 14250, Loss: 0.8591\n",
            "Iteration 14400, Loss: 0.8576\n",
            "Iteration 14550, Loss: 0.8562\n",
            "Iteration 14700, Loss: 0.8547\n",
            "Iteration 14850, Loss: 0.8533\n",
            "Iteration 15000, Loss: 0.8519\n",
            "Iteration 15150, Loss: 0.8505\n",
            "Iteration 15300, Loss: 0.8491\n",
            "Iteration 15450, Loss: 0.8477\n",
            "Iteration 15600, Loss: 0.8464\n",
            "Iteration 15750, Loss: 0.8450\n",
            "Iteration 15900, Loss: 0.8437\n",
            "Iteration 16050, Loss: 0.8424\n",
            "Iteration 16200, Loss: 0.8411\n",
            "Iteration 16350, Loss: 0.8398\n",
            "Iteration 16500, Loss: 0.8385\n",
            "Iteration 16650, Loss: 0.8372\n",
            "Iteration 16800, Loss: 0.8360\n",
            "Iteration 16950, Loss: 0.8347\n",
            "Iteration 17100, Loss: 0.8335\n",
            "Iteration 17250, Loss: 0.8323\n",
            "Iteration 17400, Loss: 0.8311\n",
            "Iteration 17550, Loss: 0.8299\n",
            "Iteration 17700, Loss: 0.8288\n",
            "Iteration 17850, Loss: 0.8276\n",
            "Iteration 18000, Loss: 0.8264\n",
            "Iteration 18150, Loss: 0.8253\n",
            "Iteration 18300, Loss: 0.8242\n",
            "Iteration 18450, Loss: 0.8231\n",
            "Iteration 18600, Loss: 0.8219\n",
            "Iteration 18750, Loss: 0.8208\n",
            "Iteration 18900, Loss: 0.8197\n",
            "Iteration 19050, Loss: 0.8187\n",
            "Iteration 19200, Loss: 0.8176\n",
            "Iteration 19350, Loss: 0.8165\n",
            "Iteration 19500, Loss: 0.8154\n",
            "Iteration 19650, Loss: 0.8144\n",
            "Iteration 19800, Loss: 0.8133\n",
            "Iteration 19950, Loss: 0.8123\n",
            "Iteration 20100, Loss: 0.8112\n",
            "Iteration 20250, Loss: 0.8102\n",
            "Iteration 20400, Loss: 0.8092\n",
            "Iteration 20550, Loss: 0.8081\n",
            "Iteration 20700, Loss: 0.8071\n",
            "Iteration 20850, Loss: 0.8061\n",
            "Iteration 21000, Loss: 0.8050\n",
            "Iteration 21150, Loss: 0.8040\n",
            "Iteration 21300, Loss: 0.8030\n",
            "Iteration 21450, Loss: 0.8020\n",
            "Iteration 21600, Loss: 0.8010\n",
            "Iteration 21750, Loss: 0.8000\n",
            "Iteration 21900, Loss: 0.7990\n",
            "Iteration 22050, Loss: 0.7979\n",
            "Iteration 22200, Loss: 0.7969\n",
            "Iteration 22350, Loss: 0.7959\n",
            "Iteration 22500, Loss: 0.7949\n",
            "Iteration 22650, Loss: 0.7939\n",
            "Iteration 22800, Loss: 0.7929\n",
            "Iteration 22950, Loss: 0.7919\n",
            "Iteration 23100, Loss: 0.7909\n",
            "Iteration 23250, Loss: 0.7899\n",
            "Iteration 23400, Loss: 0.7889\n",
            "Iteration 23550, Loss: 0.7879\n",
            "Iteration 23700, Loss: 0.7869\n",
            "Iteration 23850, Loss: 0.7859\n",
            "Iteration 24000, Loss: 0.7849\n",
            "Iteration 24150, Loss: 0.7839\n",
            "Iteration 24300, Loss: 0.7829\n",
            "Iteration 24450, Loss: 0.7819\n",
            "Iteration 24600, Loss: 0.7809\n",
            "Iteration 24750, Loss: 0.7799\n",
            "Iteration 24900, Loss: 0.7789\n",
            "Iteration 25050, Loss: 0.7779\n",
            "Iteration 25200, Loss: 0.7769\n",
            "Iteration 25350, Loss: 0.7758\n",
            "Iteration 25500, Loss: 0.7748\n",
            "Iteration 25650, Loss: 0.7738\n",
            "Iteration 25800, Loss: 0.7728\n",
            "Iteration 25950, Loss: 0.7718\n",
            "Iteration 26100, Loss: 0.7708\n",
            "Iteration 26250, Loss: 0.7698\n",
            "Iteration 26400, Loss: 0.7688\n",
            "Iteration 26550, Loss: 0.7678\n",
            "Iteration 26700, Loss: 0.7667\n",
            "Iteration 26850, Loss: 0.7657\n",
            "Iteration 27000, Loss: 0.7647\n",
            "Iteration 27150, Loss: 0.7637\n",
            "Iteration 27300, Loss: 0.7627\n",
            "Iteration 27450, Loss: 0.7617\n",
            "Iteration 27600, Loss: 0.7606\n",
            "Iteration 27750, Loss: 0.7596\n",
            "Iteration 27900, Loss: 0.7586\n",
            "Iteration 28050, Loss: 0.7576\n",
            "Iteration 28200, Loss: 0.7566\n",
            "Iteration 28350, Loss: 0.7555\n",
            "Iteration 28500, Loss: 0.7545\n",
            "Iteration 28650, Loss: 0.7535\n",
            "Iteration 28800, Loss: 0.7525\n",
            "Iteration 28950, Loss: 0.7515\n",
            "Iteration 29100, Loss: 0.7504\n",
            "Iteration 29250, Loss: 0.7494\n",
            "Iteration 29400, Loss: 0.7484\n",
            "Iteration 29550, Loss: 0.7474\n",
            "Iteration 29700, Loss: 0.7463\n",
            "Iteration 29850, Loss: 0.7453\n",
            "Iteration 30000, Loss: 0.7443\n",
            "Iteration 30150, Loss: 0.7433\n",
            "Iteration 30300, Loss: 0.7422\n",
            "Iteration 30450, Loss: 0.7412\n",
            "Iteration 30600, Loss: 0.7402\n",
            "Iteration 30750, Loss: 0.7392\n",
            "Iteration 30900, Loss: 0.7382\n",
            "Iteration 31050, Loss: 0.7371\n",
            "Iteration 31200, Loss: 0.7361\n",
            "Iteration 31350, Loss: 0.7351\n",
            "Iteration 31500, Loss: 0.7341\n",
            "Iteration 31650, Loss: 0.7331\n",
            "Iteration 31800, Loss: 0.7320\n",
            "Iteration 31950, Loss: 0.7310\n",
            "Iteration 32100, Loss: 0.7300\n",
            "Iteration 32250, Loss: 0.7290\n",
            "Iteration 32400, Loss: 0.7280\n",
            "Iteration 32550, Loss: 0.7269\n",
            "Iteration 32700, Loss: 0.7259\n",
            "Iteration 32850, Loss: 0.7249\n",
            "Iteration 33000, Loss: 0.7239\n",
            "Iteration 33150, Loss: 0.7229\n",
            "Iteration 33300, Loss: 0.7219\n",
            "Iteration 33450, Loss: 0.7209\n",
            "Iteration 33600, Loss: 0.7199\n",
            "Iteration 33750, Loss: 0.7189\n",
            "Iteration 33900, Loss: 0.7179\n",
            "Iteration 34050, Loss: 0.7169\n",
            "Iteration 34200, Loss: 0.7159\n",
            "Iteration 34350, Loss: 0.7149\n",
            "Iteration 34500, Loss: 0.7139\n",
            "Iteration 34650, Loss: 0.7129\n",
            "Iteration 34800, Loss: 0.7119\n",
            "Iteration 34950, Loss: 0.7109\n",
            "Iteration 35100, Loss: 0.7099\n",
            "Iteration 35250, Loss: 0.7089\n",
            "Iteration 35400, Loss: 0.7079\n",
            "Iteration 35550, Loss: 0.7070\n",
            "Iteration 35700, Loss: 0.7060\n",
            "Iteration 35850, Loss: 0.7050\n",
            "Iteration 36000, Loss: 0.7040\n",
            "Iteration 36150, Loss: 0.7031\n",
            "Iteration 36300, Loss: 0.7021\n",
            "Iteration 36450, Loss: 0.7011\n",
            "Iteration 36600, Loss: 0.7002\n",
            "Iteration 36750, Loss: 0.6992\n",
            "Iteration 36900, Loss: 0.6983\n",
            "Iteration 37050, Loss: 0.6973\n",
            "Iteration 37200, Loss: 0.6964\n",
            "Iteration 37350, Loss: 0.6954\n",
            "Iteration 37500, Loss: 0.6945\n",
            "Iteration 37650, Loss: 0.6935\n",
            "Iteration 37800, Loss: 0.6926\n",
            "Iteration 37950, Loss: 0.6917\n",
            "Iteration 38100, Loss: 0.6907\n",
            "Iteration 38250, Loss: 0.6898\n",
            "Iteration 38400, Loss: 0.6889\n",
            "Iteration 38550, Loss: 0.6879\n",
            "Iteration 38700, Loss: 0.6870\n",
            "Iteration 38850, Loss: 0.6861\n",
            "Iteration 39000, Loss: 0.6852\n",
            "Iteration 39150, Loss: 0.6843\n",
            "Iteration 39300, Loss: 0.6834\n",
            "Iteration 39450, Loss: 0.6825\n",
            "Iteration 39600, Loss: 0.6816\n",
            "Iteration 39750, Loss: 0.6807\n",
            "Iteration 39900, Loss: 0.6798\n",
            "Iteration 40050, Loss: 0.6789\n",
            "Iteration 40200, Loss: 0.6781\n",
            "Iteration 40350, Loss: 0.6772\n",
            "Iteration 40500, Loss: 0.6763\n",
            "Iteration 40650, Loss: 0.6754\n",
            "Iteration 40800, Loss: 0.6746\n",
            "Iteration 40950, Loss: 0.6737\n",
            "Iteration 41100, Loss: 0.6729\n",
            "Iteration 41250, Loss: 0.6720\n",
            "Iteration 41400, Loss: 0.6711\n",
            "Iteration 41550, Loss: 0.6703\n",
            "Iteration 41700, Loss: 0.6695\n",
            "Iteration 41850, Loss: 0.6686\n",
            "Iteration 42000, Loss: 0.6678\n",
            "Iteration 42150, Loss: 0.6670\n",
            "Iteration 42300, Loss: 0.6661\n",
            "Iteration 42450, Loss: 0.6653\n",
            "Iteration 42600, Loss: 0.6645\n",
            "Iteration 42750, Loss: 0.6637\n",
            "Iteration 42900, Loss: 0.6629\n",
            "Iteration 43050, Loss: 0.6621\n",
            "Iteration 43200, Loss: 0.6612\n",
            "Iteration 43350, Loss: 0.6604\n",
            "Iteration 43500, Loss: 0.6597\n",
            "Iteration 43650, Loss: 0.6589\n",
            "Iteration 43800, Loss: 0.6581\n",
            "Iteration 43950, Loss: 0.6573\n",
            "Iteration 44100, Loss: 0.6565\n",
            "Iteration 44250, Loss: 0.6557\n",
            "Iteration 44400, Loss: 0.6549\n",
            "Iteration 44550, Loss: 0.6542\n",
            "Iteration 44700, Loss: 0.6534\n",
            "Iteration 44850, Loss: 0.6526\n",
            "Iteration 45000, Loss: 0.6519\n",
            "Iteration 45150, Loss: 0.6511\n",
            "Iteration 45300, Loss: 0.6504\n",
            "Iteration 45450, Loss: 0.6496\n",
            "Iteration 45600, Loss: 0.6489\n",
            "Iteration 45750, Loss: 0.6481\n",
            "Iteration 45900, Loss: 0.6474\n",
            "Iteration 46050, Loss: 0.6466\n",
            "Iteration 46200, Loss: 0.6459\n",
            "Iteration 46350, Loss: 0.6452\n",
            "Iteration 46500, Loss: 0.6444\n",
            "Iteration 46650, Loss: 0.6437\n",
            "Iteration 46800, Loss: 0.6430\n",
            "Iteration 46950, Loss: 0.6422\n",
            "Iteration 47100, Loss: 0.6415\n",
            "Iteration 47250, Loss: 0.6408\n",
            "Iteration 47400, Loss: 0.6401\n",
            "Iteration 47550, Loss: 0.6394\n",
            "Iteration 47700, Loss: 0.6387\n",
            "Iteration 47850, Loss: 0.6379\n",
            "Iteration 48000, Loss: 0.6372\n",
            "Iteration 48150, Loss: 0.6365\n",
            "Iteration 48300, Loss: 0.6358\n",
            "Iteration 48450, Loss: 0.6351\n",
            "Iteration 48600, Loss: 0.6344\n",
            "Iteration 48750, Loss: 0.6337\n",
            "Iteration 48900, Loss: 0.6330\n",
            "Iteration 49050, Loss: 0.6323\n",
            "Iteration 49200, Loss: 0.6316\n",
            "Iteration 49350, Loss: 0.6309\n",
            "Iteration 49500, Loss: 0.6302\n",
            "Iteration 49650, Loss: 0.6296\n",
            "Iteration 49800, Loss: 0.6289\n",
            "Iteration 49950, Loss: 0.6282\n",
            "Iteration 50100, Loss: 0.6275\n",
            "Iteration 50250, Loss: 0.6268\n",
            "Iteration 50400, Loss: 0.6261\n",
            "Iteration 50550, Loss: 0.6254\n",
            "Iteration 50700, Loss: 0.6247\n",
            "Iteration 50850, Loss: 0.6240\n",
            "Iteration 51000, Loss: 0.6234\n",
            "Iteration 51150, Loss: 0.6227\n",
            "Iteration 51300, Loss: 0.6220\n",
            "Iteration 51450, Loss: 0.6213\n",
            "Iteration 51600, Loss: 0.6206\n",
            "Iteration 51750, Loss: 0.6199\n",
            "Iteration 51900, Loss: 0.6192\n",
            "Iteration 52050, Loss: 0.6186\n",
            "Iteration 52200, Loss: 0.6179\n",
            "Iteration 52350, Loss: 0.6172\n",
            "Iteration 52500, Loss: 0.6165\n",
            "Iteration 52650, Loss: 0.6158\n",
            "Iteration 52800, Loss: 0.6151\n",
            "Iteration 52950, Loss: 0.6144\n",
            "Iteration 53100, Loss: 0.6137\n",
            "Iteration 53250, Loss: 0.6130\n",
            "Iteration 53400, Loss: 0.6123\n",
            "Iteration 53550, Loss: 0.6117\n",
            "Iteration 53700, Loss: 0.6110\n",
            "Iteration 53850, Loss: 0.6103\n",
            "Iteration 54000, Loss: 0.6096\n",
            "Iteration 54150, Loss: 0.6089\n",
            "Iteration 54300, Loss: 0.6082\n",
            "Iteration 54450, Loss: 0.6075\n",
            "Iteration 54600, Loss: 0.6068\n",
            "Iteration 54750, Loss: 0.6061\n",
            "Iteration 54900, Loss: 0.6054\n",
            "Iteration 55050, Loss: 0.6046\n",
            "Iteration 55200, Loss: 0.6039\n",
            "Iteration 55350, Loss: 0.6032\n",
            "Iteration 55500, Loss: 0.6025\n",
            "Iteration 55650, Loss: 0.6018\n",
            "Iteration 55800, Loss: 0.6011\n",
            "Iteration 55950, Loss: 0.6004\n",
            "Iteration 56100, Loss: 0.5996\n",
            "Iteration 56250, Loss: 0.5989\n",
            "Iteration 56400, Loss: 0.5982\n",
            "Iteration 56550, Loss: 0.5975\n",
            "Iteration 56700, Loss: 0.5967\n",
            "Iteration 56850, Loss: 0.5960\n",
            "Iteration 57000, Loss: 0.5953\n",
            "Iteration 57150, Loss: 0.5945\n",
            "Iteration 57300, Loss: 0.5938\n",
            "Iteration 57450, Loss: 0.5930\n",
            "Iteration 57600, Loss: 0.5923\n",
            "Iteration 57750, Loss: 0.5915\n",
            "Iteration 57900, Loss: 0.5908\n",
            "Iteration 58050, Loss: 0.5900\n",
            "Iteration 58200, Loss: 0.5893\n",
            "Iteration 58350, Loss: 0.5885\n",
            "Iteration 58500, Loss: 0.5878\n",
            "Iteration 58650, Loss: 0.5870\n",
            "Iteration 58800, Loss: 0.5862\n",
            "Iteration 58950, Loss: 0.5855\n",
            "Iteration 59100, Loss: 0.5847\n",
            "Iteration 59250, Loss: 0.5839\n",
            "Iteration 59400, Loss: 0.5832\n",
            "Iteration 59550, Loss: 0.5824\n",
            "Iteration 59700, Loss: 0.5816\n",
            "Iteration 59850, Loss: 0.5808\n",
            "Iteration 60000, Loss: 0.5800\n",
            "Iteration 60150, Loss: 0.5792\n",
            "Iteration 60300, Loss: 0.5785\n",
            "Iteration 60450, Loss: 0.5777\n",
            "Iteration 60600, Loss: 0.5769\n",
            "Iteration 60750, Loss: 0.5761\n",
            "Iteration 60900, Loss: 0.5753\n",
            "Iteration 61050, Loss: 0.5745\n",
            "Iteration 61200, Loss: 0.5737\n",
            "Iteration 61350, Loss: 0.5729\n",
            "Iteration 61500, Loss: 0.5720\n",
            "Iteration 61650, Loss: 0.5712\n",
            "Iteration 61800, Loss: 0.5704\n",
            "Iteration 61950, Loss: 0.5696\n",
            "Iteration 62100, Loss: 0.5688\n",
            "Iteration 62250, Loss: 0.5680\n",
            "Iteration 62400, Loss: 0.5671\n",
            "Iteration 62550, Loss: 0.5663\n",
            "Iteration 62700, Loss: 0.5655\n",
            "Iteration 62850, Loss: 0.5647\n",
            "Iteration 63000, Loss: 0.5638\n",
            "Iteration 63150, Loss: 0.5630\n",
            "Iteration 63300, Loss: 0.5622\n",
            "Iteration 63450, Loss: 0.5613\n",
            "Iteration 63600, Loss: 0.5605\n",
            "Iteration 63750, Loss: 0.5597\n",
            "Iteration 63900, Loss: 0.5588\n",
            "Iteration 64050, Loss: 0.5580\n",
            "Iteration 64200, Loss: 0.5571\n",
            "Iteration 64350, Loss: 0.5563\n",
            "Iteration 64500, Loss: 0.5555\n",
            "Iteration 64650, Loss: 0.5546\n",
            "Iteration 64800, Loss: 0.5538\n",
            "Iteration 64950, Loss: 0.5529\n",
            "Iteration 65100, Loss: 0.5521\n",
            "Iteration 65250, Loss: 0.5512\n",
            "Iteration 65400, Loss: 0.5504\n",
            "Iteration 65550, Loss: 0.5495\n",
            "Iteration 65700, Loss: 0.5487\n",
            "Iteration 65850, Loss: 0.5478\n",
            "Iteration 66000, Loss: 0.5470\n",
            "Iteration 66150, Loss: 0.5461\n",
            "Iteration 66300, Loss: 0.5453\n",
            "Iteration 66450, Loss: 0.5444\n",
            "Iteration 66600, Loss: 0.5436\n",
            "Iteration 66750, Loss: 0.5427\n",
            "Iteration 66900, Loss: 0.5419\n",
            "Iteration 67050, Loss: 0.5410\n",
            "Iteration 67200, Loss: 0.5402\n",
            "Iteration 67350, Loss: 0.5393\n",
            "Iteration 67500, Loss: 0.5385\n",
            "Iteration 67650, Loss: 0.5376\n",
            "Iteration 67800, Loss: 0.5367\n",
            "Iteration 67950, Loss: 0.5359\n",
            "Iteration 68100, Loss: 0.5350\n",
            "Iteration 68250, Loss: 0.5342\n",
            "Iteration 68400, Loss: 0.5333\n",
            "Iteration 68550, Loss: 0.5325\n",
            "Iteration 68700, Loss: 0.5316\n",
            "Iteration 68850, Loss: 0.5308\n",
            "Iteration 69000, Loss: 0.5300\n",
            "Iteration 69150, Loss: 0.5291\n",
            "Iteration 69300, Loss: 0.5283\n",
            "Iteration 69450, Loss: 0.5274\n",
            "Iteration 69600, Loss: 0.5266\n",
            "Iteration 69750, Loss: 0.5257\n",
            "Iteration 69900, Loss: 0.5249\n",
            "Iteration 70050, Loss: 0.5241\n",
            "Iteration 70200, Loss: 0.5232\n",
            "Iteration 70350, Loss: 0.5224\n",
            "Iteration 70500, Loss: 0.5215\n",
            "Iteration 70650, Loss: 0.5207\n",
            "Iteration 70800, Loss: 0.5199\n",
            "Iteration 70950, Loss: 0.5190\n",
            "Iteration 71100, Loss: 0.5182\n",
            "Iteration 71250, Loss: 0.5174\n",
            "Iteration 71400, Loss: 0.5166\n",
            "Iteration 71550, Loss: 0.5157\n",
            "Iteration 71700, Loss: 0.5149\n",
            "Iteration 71850, Loss: 0.5141\n",
            "Iteration 72000, Loss: 0.5133\n",
            "Iteration 72150, Loss: 0.5124\n",
            "Iteration 72300, Loss: 0.5116\n",
            "Iteration 72450, Loss: 0.5108\n",
            "Iteration 72600, Loss: 0.5100\n",
            "Iteration 72750, Loss: 0.5092\n",
            "Iteration 72900, Loss: 0.5084\n",
            "Iteration 73050, Loss: 0.5076\n",
            "Iteration 73200, Loss: 0.5067\n",
            "Iteration 73350, Loss: 0.5059\n",
            "Iteration 73500, Loss: 0.5051\n",
            "Iteration 73650, Loss: 0.5043\n",
            "Iteration 73800, Loss: 0.5035\n",
            "Iteration 73950, Loss: 0.5027\n",
            "Iteration 74100, Loss: 0.5019\n",
            "Iteration 74250, Loss: 0.5011\n",
            "Iteration 74400, Loss: 0.5003\n",
            "Iteration 74550, Loss: 0.4996\n",
            "Iteration 74700, Loss: 0.4988\n",
            "Iteration 74850, Loss: 0.4980\n",
            "Iteration 75000, Loss: 0.4972\n",
            "Iteration 75150, Loss: 0.4964\n",
            "Iteration 75300, Loss: 0.4956\n",
            "Iteration 75450, Loss: 0.4949\n",
            "Iteration 75600, Loss: 0.4941\n",
            "Iteration 75750, Loss: 0.4933\n",
            "Iteration 75900, Loss: 0.4925\n",
            "Iteration 76050, Loss: 0.4918\n",
            "Iteration 76200, Loss: 0.4910\n",
            "Iteration 76350, Loss: 0.4902\n",
            "Iteration 76500, Loss: 0.4895\n",
            "Iteration 76650, Loss: 0.4887\n",
            "Iteration 76800, Loss: 0.4879\n",
            "Iteration 76950, Loss: 0.4872\n",
            "Iteration 77100, Loss: 0.4864\n",
            "Iteration 77250, Loss: 0.4857\n",
            "Iteration 77400, Loss: 0.4849\n",
            "Iteration 77550, Loss: 0.4842\n",
            "Iteration 77700, Loss: 0.4834\n",
            "Iteration 77850, Loss: 0.4827\n",
            "Iteration 78000, Loss: 0.4819\n",
            "Iteration 78150, Loss: 0.4812\n",
            "Iteration 78300, Loss: 0.4804\n",
            "Iteration 78450, Loss: 0.4797\n",
            "Iteration 78600, Loss: 0.4790\n",
            "Iteration 78750, Loss: 0.4782\n",
            "Iteration 78900, Loss: 0.4775\n",
            "Iteration 79050, Loss: 0.4768\n",
            "Iteration 79200, Loss: 0.4760\n",
            "Iteration 79350, Loss: 0.4753\n",
            "Iteration 79500, Loss: 0.4746\n",
            "Iteration 79650, Loss: 0.4739\n",
            "Iteration 79800, Loss: 0.4732\n",
            "Iteration 79950, Loss: 0.4724\n",
            "Iteration 80100, Loss: 0.4717\n",
            "Iteration 80250, Loss: 0.4710\n",
            "Iteration 80400, Loss: 0.4703\n",
            "Iteration 80550, Loss: 0.4696\n",
            "Iteration 80700, Loss: 0.4689\n",
            "Iteration 80850, Loss: 0.4682\n",
            "Iteration 81000, Loss: 0.4675\n",
            "Iteration 81150, Loss: 0.4668\n",
            "Iteration 81300, Loss: 0.4661\n",
            "Iteration 81450, Loss: 0.4654\n",
            "Iteration 81600, Loss: 0.4647\n",
            "Iteration 81750, Loss: 0.4640\n",
            "Iteration 81900, Loss: 0.4633\n",
            "Iteration 82050, Loss: 0.4626\n",
            "Iteration 82200, Loss: 0.4620\n",
            "Iteration 82350, Loss: 0.4613\n",
            "Iteration 82500, Loss: 0.4606\n",
            "Iteration 82650, Loss: 0.4599\n",
            "Iteration 82800, Loss: 0.4593\n",
            "Iteration 82950, Loss: 0.4586\n",
            "Iteration 83100, Loss: 0.4579\n",
            "Iteration 83250, Loss: 0.4572\n",
            "Iteration 83400, Loss: 0.4566\n",
            "Iteration 83550, Loss: 0.4559\n",
            "Iteration 83700, Loss: 0.4552\n",
            "Iteration 83850, Loss: 0.4546\n",
            "Iteration 84000, Loss: 0.4539\n",
            "Iteration 84150, Loss: 0.4533\n",
            "Iteration 84300, Loss: 0.4526\n",
            "Iteration 84450, Loss: 0.4520\n",
            "Iteration 84600, Loss: 0.4513\n",
            "Iteration 84750, Loss: 0.4507\n",
            "Iteration 84900, Loss: 0.4500\n",
            "Iteration 85050, Loss: 0.4494\n",
            "Iteration 85200, Loss: 0.4488\n",
            "Iteration 85350, Loss: 0.4481\n",
            "Iteration 85500, Loss: 0.4475\n",
            "Iteration 85650, Loss: 0.4468\n",
            "Iteration 85800, Loss: 0.4462\n",
            "Iteration 85950, Loss: 0.4456\n",
            "Iteration 86100, Loss: 0.4450\n",
            "Iteration 86250, Loss: 0.4443\n",
            "Iteration 86400, Loss: 0.4437\n",
            "Iteration 86550, Loss: 0.4431\n",
            "Iteration 86700, Loss: 0.4425\n",
            "Iteration 86850, Loss: 0.4419\n",
            "Iteration 87000, Loss: 0.4412\n",
            "Iteration 87150, Loss: 0.4406\n",
            "Iteration 87300, Loss: 0.4400\n",
            "Iteration 87450, Loss: 0.4394\n",
            "Iteration 87600, Loss: 0.4388\n",
            "Iteration 87750, Loss: 0.4382\n",
            "Iteration 87900, Loss: 0.4376\n",
            "Iteration 88050, Loss: 0.4370\n",
            "Iteration 88200, Loss: 0.4364\n",
            "Iteration 88350, Loss: 0.4358\n",
            "Iteration 88500, Loss: 0.4352\n",
            "Iteration 88650, Loss: 0.4346\n",
            "Iteration 88800, Loss: 0.4340\n",
            "Iteration 88950, Loss: 0.4334\n",
            "Iteration 89100, Loss: 0.4328\n",
            "Iteration 89250, Loss: 0.4323\n",
            "Iteration 89400, Loss: 0.4317\n",
            "Iteration 89550, Loss: 0.4311\n",
            "Iteration 89700, Loss: 0.4305\n",
            "Iteration 89850, Loss: 0.4299\n",
            "Iteration 90000, Loss: 0.4294\n",
            "Iteration 90150, Loss: 0.4288\n",
            "Iteration 90300, Loss: 0.4282\n",
            "Iteration 90450, Loss: 0.4276\n",
            "Iteration 90600, Loss: 0.4271\n",
            "Iteration 90750, Loss: 0.4265\n",
            "Iteration 90900, Loss: 0.4259\n",
            "Iteration 91050, Loss: 0.4254\n",
            "Iteration 91200, Loss: 0.4248\n",
            "Iteration 91350, Loss: 0.4243\n",
            "Iteration 91500, Loss: 0.4237\n",
            "Iteration 91650, Loss: 0.4232\n",
            "Iteration 91800, Loss: 0.4226\n",
            "Iteration 91950, Loss: 0.4220\n",
            "Iteration 92100, Loss: 0.4215\n",
            "Iteration 92250, Loss: 0.4210\n",
            "Iteration 92400, Loss: 0.4204\n",
            "Iteration 92550, Loss: 0.4199\n",
            "Iteration 92700, Loss: 0.4193\n",
            "Iteration 92850, Loss: 0.4188\n",
            "Iteration 93000, Loss: 0.4182\n",
            "Iteration 93150, Loss: 0.4177\n",
            "Iteration 93300, Loss: 0.4172\n",
            "Iteration 93450, Loss: 0.4166\n",
            "Iteration 93600, Loss: 0.4161\n",
            "Iteration 93750, Loss: 0.4156\n",
            "Iteration 93900, Loss: 0.4150\n",
            "Iteration 94050, Loss: 0.4145\n",
            "Iteration 94200, Loss: 0.4140\n",
            "Iteration 94350, Loss: 0.4135\n",
            "Iteration 94500, Loss: 0.4129\n",
            "Iteration 94650, Loss: 0.4124\n",
            "Iteration 94800, Loss: 0.4119\n",
            "Iteration 94950, Loss: 0.4114\n",
            "Iteration 95100, Loss: 0.4109\n",
            "Iteration 95250, Loss: 0.4103\n",
            "Iteration 95400, Loss: 0.4098\n",
            "Iteration 95550, Loss: 0.4093\n",
            "Iteration 95700, Loss: 0.4088\n",
            "Iteration 95850, Loss: 0.4083\n",
            "Iteration 96000, Loss: 0.4078\n",
            "Iteration 96150, Loss: 0.4073\n",
            "Iteration 96300, Loss: 0.4068\n",
            "Iteration 96450, Loss: 0.4063\n",
            "Iteration 96600, Loss: 0.4058\n",
            "Iteration 96750, Loss: 0.4053\n",
            "Iteration 96900, Loss: 0.4048\n",
            "Iteration 97050, Loss: 0.4043\n",
            "Iteration 97200, Loss: 0.4038\n",
            "Iteration 97350, Loss: 0.4033\n",
            "Iteration 97500, Loss: 0.4028\n",
            "Iteration 97650, Loss: 0.4023\n",
            "Iteration 97800, Loss: 0.4018\n",
            "Iteration 97950, Loss: 0.4013\n",
            "Iteration 98100, Loss: 0.4008\n",
            "Iteration 98250, Loss: 0.4004\n",
            "Iteration 98400, Loss: 0.3999\n",
            "Iteration 98550, Loss: 0.3994\n",
            "Iteration 98700, Loss: 0.3989\n",
            "Iteration 98850, Loss: 0.3984\n",
            "Iteration 99000, Loss: 0.3979\n",
            "Iteration 99150, Loss: 0.3975\n",
            "Iteration 99300, Loss: 0.3970\n",
            "Iteration 99450, Loss: 0.3965\n",
            "Iteration 99600, Loss: 0.3960\n",
            "Iteration 99750, Loss: 0.3956\n",
            "Iteration 99900, Loss: 0.3951\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_multiclass(X, params):\n",
        "  params = forward_pass(X, params)\n",
        "\n",
        "  predictions = np.argmax(params['A2'], axis=1)\n",
        "  return predictions\n"
      ],
      "metadata": {
        "id": "E2YhFaQGV2F6"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def confusion_matrix_multiclass(y_true, y_pred):\n",
        "  K = len(np.unique(y_true))\n",
        "  result = np.zeros((K, K))\n",
        "\n",
        "  for i in range(len(y_true)):\n",
        "    result[y_true[i]-1][y_pred[i]-1] += 1\n",
        "\n",
        "  return result"
      ],
      "metadata": {
        "id": "Iynav_3rcpRS"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def precision_recall_f1_multiclass(y_true, y_pred):\n",
        "\n",
        "  c_matrix = confusion_matrix_multiclass(y_true, y_pred)\n",
        "  results = []\n",
        "  for i in range(c_matrix.shape[0]):\n",
        "    tp = c_matrix[i, i]\n",
        "    fp = c_matrix[:, i].sum() - tp\n",
        "    fn = c_matrix[i, :].sum() - tp\n",
        "    precision = tp / (tp + fp)\n",
        "    recall = tp / (tp + fn)\n",
        "    f1 = 2 * (precision * recall) / (precision + recall)\n",
        "    results.append({\n",
        "                    'class': i+1,\n",
        "                    'precision': round(precision, 3), 'recall': round(recall, 3),\n",
        "                    'f1': round(f1, 3)\n",
        "                    })\n",
        "  return results"
      ],
      "metadata": {
        "id": "1Fczm4hqc6IP"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_pred = predict_multiclass(X_test, params)"
      ],
      "metadata": {
        "id": "B4cOBygSV2Ht"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Glass dataset metrics with sigmoid:\")\n",
        "\n",
        "res_glass = precision_recall_f1_multiclass(Y_test.to_numpy().squeeze(), Y_pred.squeeze())\n",
        "\n",
        "print(\"GLASS = \")\n",
        "for res in res_glass:\n",
        "  print(res)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RREx_tJBdHMl",
        "outputId": "1dad5de2-ff92-4c20-a0c1-5a8f6783b6c5"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Glass dataset metrics with sigmoid:\n",
            "GLASS = \n",
            "{'class': 1, 'precision': 0.9, 'recall': 0.818, 'f1': 0.857}\n",
            "{'class': 2, 'precision': 0.812, 'recall': 0.929, 'f1': 0.867}\n",
            "{'class': 3, 'precision': 0.5, 'recall': 0.333, 'f1': 0.4}\n",
            "{'class': 4, 'precision': 1.0, 'recall': 0.875, 'f1': 0.933}\n",
            "{'class': 5, 'precision': 0.75, 'recall': 0.75, 'f1': 0.75}\n",
            "{'class': 6, 'precision': 0.75, 'recall': 1.0, 'f1': 0.857}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Glass dataset metrics with sigmoid and more hidden layers:\")\n",
        "\n",
        "res_glass = precision_recall_f1_multiclass(Y_test.to_numpy().squeeze(), Y_pred.squeeze())\n",
        "\n",
        "print(\"GLASS = \")\n",
        "for res in res_glass:\n",
        "  print(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SR2CflYC25uk",
        "outputId": "ad4e6c5a-02de-467a-a871-ffb3f18c727b"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Glass dataset metrics with sigmoid and mor ehidden layers:\n",
            "GLASS = \n",
            "{'class': 1, 'precision': 0.727, 'recall': 0.727, 'f1': 0.727}\n",
            "{'class': 2, 'precision': 0.714, 'recall': 0.714, 'f1': 0.714}\n",
            "{'class': 3, 'precision': 0.5, 'recall': 0.333, 'f1': 0.4}\n",
            "{'class': 4, 'precision': 0.875, 'recall': 0.875, 'f1': 0.875}\n",
            "{'class': 5, 'precision': 0.75, 'recall': 0.75, 'f1': 0.75}\n",
            "{'class': 6, 'precision': 0.75, 'recall': 1.0, 'f1': 0.857}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Результат тільки погіршився."
      ],
      "metadata": {
        "id": "mU4KszJb3HWc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Glass dataset metrics with relu:\")\n",
        "\n",
        "res_glass = precision_recall_f1_multiclass(Y_test.to_numpy().squeeze(), Y_pred.squeeze())\n",
        "\n",
        "print(\"GLASS = \")\n",
        "for res in res_glass:\n",
        "  print(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvSli03Vyt2J",
        "outputId": "d2aef01e-9050-4a7e-e226-2d1602bfecc0"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Glass dataset metrics with relu:\n",
            "GLASS = \n",
            "{'class': 1, 'precision': 0.727, 'recall': 0.727, 'f1': 0.727}\n",
            "{'class': 2, 'precision': 0.769, 'recall': 0.714, 'f1': 0.741}\n",
            "{'class': 3, 'precision': 0.5, 'recall': 0.667, 'f1': 0.571}\n",
            "{'class': 4, 'precision': 0.889, 'recall': 1.0, 'f1': 0.941}\n",
            "{'class': 5, 'precision': 0.5, 'recall': 0.5, 'f1': 0.5}\n",
            "{'class': 6, 'precision': 1.0, 'recall': 0.667, 'f1': 0.8}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Клас 1: Точність (precision), повнота (recall), і F1-скор знизились зі зміною на ReLU. Це може свідчити про те, що для цього класу сигмоїда давала більш м'яку границю рішення, що краще підходила під розподіл даних.\n",
        "\n",
        "Клас 2: Схожа ситуація, де всі метрики знизились. Це може бути ознакою того, що лінійність ReLU не дозволяє ефективно розділити класи, які могли бути нелінійно розділені сигмоїдою.\n",
        "\n",
        "Клас 3: Незважаючи на зниження точності, повнота і F1-скор покращились. Це може означати, що ReLU краще виявляє позитивні випадки для цього класу.\n",
        "\n",
        "Клас 4: Всі показники високі і F1-скор навіть трохи покращився з ReLU. Можливо, ReLU допомагає краще розділити цей клас від інших.\n",
        "\n",
        "Клас 5 і 6: Точність і повнота знизились для класу 5, але точність значно покращилась для класу 6 зі зменшенням повноти."
      ],
      "metadata": {
        "id": "WLQni9945UE4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Glass dataset metrics with tanh:\")\n",
        "\n",
        "res_glass = precision_recall_f1_multiclass(Y_test.to_numpy().squeeze(), Y_pred.squeeze())\n",
        "\n",
        "print(\"GLASS = \")\n",
        "for res in res_glass:\n",
        "  print(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29cDU3vvyt9h",
        "outputId": "bc24d25d-cf5a-482c-8893-68b4ecb1cb21"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Glass dataset metrics with tanh:\n",
            "GLASS = \n",
            "{'class': 1, 'precision': 0.75, 'recall': 0.818, 'f1': 0.783}\n",
            "{'class': 2, 'precision': 0.846, 'recall': 0.786, 'f1': 0.815}\n",
            "{'class': 3, 'precision': 0.667, 'recall': 0.667, 'f1': 0.667}\n",
            "{'class': 4, 'precision': 1.0, 'recall': 0.875, 'f1': 0.933}\n",
            "{'class': 5, 'precision': 0.75, 'recall': 0.75, 'f1': 0.75}\n",
            "{'class': 6, 'precision': 0.75, 'recall': 1.0, 'f1': 0.857}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Порівнявши результати з функцією активації сигмоїд та гіперболічного тангенса (tanh), можна зробити наступні спостереження:\n",
        "\n",
        "Клас 1: Після використання tanh точність знизилась, але повнота залишилась незмінною. F1-скор в результаті зменшився. Це може свідчити про те, що tanh не зміг так само ефективно розділити цей клас як сигмоїд.\n",
        "\n",
        "Клас 2: Точність підвищилась при використанні tanh, однак повнота знизилась. Загалом F1-скор трохи знизився, що вказує на те, що tanh може бути більш селективним, але менш чутливим.\n",
        "\n",
        "Клас 3: Значення всіх метрик підвищилися з використанням tanh, що може бути вказівкою на те, що ця функція активації краще впоралася з розділенням цього класу.\n",
        "\n",
        "Клас 4: Метрики залишилися незмінними, що свідчить про те, що обидві функції активації однаково ефективно працюють з цим класом.\n",
        "\n",
        "Клас 5: Не було змін у метриках між tanh та сигмоїдою, що показує, що обидві функції активації мають схожу ефективність для цього класу.\n",
        "\n",
        "Клас 6: Метрики залишилися стабільними, як і при використанні сигмоїдної функції активації.\n",
        "\n",
        "Загальний висновок полягає в тому, що використання tanh не привело до значного покращення результатів моделі. У деяких класах було невелике підвищення точності або повноти, але в цілому вплив був досить мінімальним або навіть негативним (як у випадку з класом 1). Це підкреслює важливість вибору відповідної функції активації на основі специфіки даних та задачі класифікації, а також необхідність експериментування з різними архітектурами та гіперпараметрами для досягнення оптимальної продуктивності."
      ],
      "metadata": {
        "id": "Qbs7kQfd3842"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(losses)\n",
        "\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss for Glass Identification Dataset')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "0iBSp7k0dJiF",
        "outputId": "8c21cc37-9980-45ef-cec4-18de99bc727d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "clf = MLPClassifier(hidden_layer_sizes=(n_hidden,), max_iter=1000, alpha=0.01, solver='sgd', verbose=100, random_state=42, learning_rate_init=.01)\n",
        "\n",
        "clf.fit(X_train, Y_train.to_numpy().ravel())\n",
        "\n",
        "Y_pred_sklearn = clf.predict(X_test)"
      ],
      "metadata": {
        "id": "w6bh5sezdxfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res_glass_sklearn = precision_recall_f1_multiclass(Y_test.to_numpy().squeeze(), Y_pred_sklearn.squeeze())\n",
        "\n",
        "print(\"GLASS SKLEARN = \")\n",
        "for res in res_glass_sklearn:\n",
        "  print(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDsfcN2Iq8vQ",
        "outputId": "40a6d958-dbe5-4139-9748-3c2e6b2a94da"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GLASS SKLEARN = \n",
            "{'class': 1, 'precision': 0.769, 'recall': 0.909, 'f1': 0.833}\n",
            "{'class': 2, 'precision': 0.786, 'recall': 0.786, 'f1': 0.786}\n",
            "{'class': 3, 'precision': 0.5, 'recall': 0.333, 'f1': 0.4}\n",
            "{'class': 4, 'precision': 1.0, 'recall': 1.0, 'f1': 1.0}\n",
            "{'class': 5, 'precision': 0.5, 'recall': 0.5, 'f1': 0.5}\n",
            "{'class': 6, 'precision': 1.0, 'recall': 0.667, 'f1': 0.8}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Клас 1: Моя модель має вищу точність (precision), але нижчу повноту (recall) порівняно з моделлю sklearn. Моя модель краще уникає помилкових позитивних результатів для класу 1, але пропускає більше реальних позитивних результатів.\n",
        "\n",
        "Клас 2: Точність моєї моделі трохи вища, а повнота значно вища, що свідчить про краще виявлення справжніх позитивних результатів для класу 2, хоча і з деякою кількістю помилкових позитивних.\n",
        "\n",
        "Клас 3: Обидві моделі показують однакові результати. Можливо, це найважчий клас для класифікації, і його покращення потребує додаткових даних або кращої обробки особливостей.\n",
        "\n",
        "Клас 4: Обидві моделі ідентифікували всі зразки класу 4 правильно (у sklearn моделі і повнота, і точність дорівнюють 1), але моя модель має трохи меншу повноту, що може вказувати на один пропущений справжній позитивний результат.\n",
        "\n",
        "Клас 5: Моя модель показує кращі результати за всіма показниками порівняно з моделлю sklearn. Це може свідчити про те, що моя модель краще налаштована на визначення цього класу.\n",
        "\n",
        "Клас 6: Моя модель має нижчу точність, але вищу повноту, ніж модель sklearn. Це означає, що моя модель краще виявляє справжні позитивні результати, але також допускає більше помилкових позитивних.\n",
        "\n",
        "Загалом, моя модель здається більш збалансованою в термінах повноти, особливо в класах, де sklearn модель має недоліки. Точність моєї моделі також є конкурентною, крім класу 6. Це може свідчити про те, що моя модель може бути більш корисною в ситуаціях, де важливіше зменшити кількість пропущених справжніх позитивних результатів, ніж помилкових позитивних."
      ],
      "metadata": {
        "id": "XNpjVbmDsYcz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Аналіз помилок.\n",
        "\n",
        "\n",
        "Загалом при будь-яких функціях активації постійно бачимо тотально поганий результат для 3го класу. Майже бездоганний для 4. Тобто, можна бачити, що якість залежить від кількості даних, адже в даному датасеті є явний дизбаланс. Дивним було те, що при релу активації якість для третього класу сильно підвищилася, хоча для інших впала. Тому, для бездоганного результата треба більш грунтовно змінювати архітектуру мережі."
      ],
      "metadata": {
        "id": "9vn9CpspPIsI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Нейронні мережі — це математичні моделі, які імітують роботу людського мозку, використовуючи велику кількість взаємопов'язаних простих обчислювальних елементів, що звуться \"нейронами\".\n",
        "\n",
        "Основні елементи нейронної мережі включають:\n",
        "1. Вхідний шар (Input Layer): Це шар, який приймає вхідні дані. Кількість нейронів у вхідному шарі зазвичай відповідає кількості ознак вхідних даних.\n",
        "2. Приховані шари (Hidden Layers): Це шари між вхідним і вихідним шарами. Нейронна мережа може мати один або кілька прихованих шарів. Кожен прихований шар складається з декількох нейронів, кожен з яких виконує певну лінійну та не лінійну обчислювальну операцію.\n",
        "3. Вихідний шар (Output Layer): Це останній шар у нейронній мережі, який видає результати передбачення. Кількість нейронів у вихідному шарі відповідає кількості цільових класів для задачі класифікації.\n",
        "4. Ваги та зміщення (Weights and Biases): Ваги - це параметри, що визначають силу впливу кожного вхідного нейрона на кожен вихідний нейрон. Зміщення - це додаткові параметри, які дозволяють кожному нейрону робити більш гнучкі передбачення.\n",
        "5. Функції активації (Activation Functions): Функції активації використовуються для введення нелінійності в нейронну мережу, що дозволяє їй моделювати більш складні залежності. Найпоширенішими функціями активації є ReLU (Rectified Linear Unit), сигмоїдна функція, та softmax-функція."
      ],
      "metadata": {
        "id": "9yjpzGpLQOue"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Градієнтний спуск є основним методом оптимізації, використовуваним для тренування нейронних мереж. Градієнтний спуск працює, оновлюючи параметри моделі (ваги і зміщення) таким чином, що мінімізує помилку передбачення моделі на тренувальних даних. Це робиться шляхом визначення градієнта (або нахилу) функції втрат моделі з відношенням до параметрів, і оновлення параметрів у протилежному напрямку до градієнта."
      ],
      "metadata": {
        "id": "PiWA0xk-QOwx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. В обох випадках процес тренування нейронної мережі майже однаковий. Головна відмінність полягає в тому, як обчислюються втрати та яка активаційна функція використовується в останньому шарі мережі."
      ],
      "metadata": {
        "id": "VstTpnTiQvMS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. В нейронних мережах використовуються різні активаційні функції для введення нелінійності в процес обчислення, що дозволяє мережі вчитися складніші шаблони і функції, які не можуть бути представлені лінійними моделями. До популярних активаційних функцій належать:\n",
        "\n",
        " - Сигмоїда: Виводить значення між 0 та 1, що є корисним для моделювання ймовірності в задачах бінарної класифікації.\n",
        " - Гіперболічний тангенс (tanh): Виводить значення між -1 та 1, центруючи дані таким чином, що негативні вхідні значення відображаються на негативний вихід.\n",
        " - ReLU (Rectified Linear Unit): Виводить 0 для негативних значень та зберігає позитивні значення без змін, що прискорює навчання та зменшує ймовірність зникнення градієнтів.\n",
        " - Leaky ReLU: Варіація ReLU, яка дозволяє малому градієнту \"просочуватися\" через негативні значення, запобігаючи проблемі вмираючих нейронів.\n",
        " - Softmax: Використовується у вихідному шарі мережі для многокласової класифікації, виводячи ймовірність кожного класу таким чином, що сума ймовірностей усіх класів дорівнює 1.\n",
        "\n",
        " Кожна з цих функцій має свої особливості та найкраще підходить для різних типів задач та архітектур нейронних мереж. Вибір активаційної функції може суттєво вплинути на ефективність навчання та здатність моделі до"
      ],
      "metadata": {
        "id": "jxDMKnf1QvOW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Функція втрат – це метрика, що використовується для кількісної оцінки того, наскільки добре модель передбачає вихідні дані. Вона вимірює різницю між передбаченнями моделі та істинними значеннями.\n",
        "Функція втрат Cross-Entropy Loss використовується у задачах класифікації для порівняння прогнозованих ймовірностей із справжніми мітками.\n",
        "\n",
        "Для задач класифікації часто використовуються такі функції втрат:\n",
        "\n",
        " - Перехресна ентропія (Cross-Entropy Loss): Це найпоширеніша функція втрат для задач бінарної та багатокласової класифікації. Вона вимірює розбіжність між фактичними мітками та прогнозованими ймовірностями, прагнучи мінімізувати цю розбіжність.\n",
        "\n",
        " - Функція втрат з маржою (Hinge Loss): Часто використовується з методами підтримки векторних машин (SVM) для задач класифікації. Вона вимірює помилку з урахуванням визначеного порогу (маржі).\n",
        "\n",
        " - Функція втрат категоричної перехресної ентропії (Categorical Cross-Entropy Loss): Розширення перехресної ентропії для багатокласових задач класифікації, де кожен вихід мережі представляє ймовірність належності до конкретного класу.\n",
        "\n",
        " - Sparse Categorical Cross-Entropy Loss: Використовується, коли фактичні мітки представлені як цілі числа, а не one-hot вектори. Це ефективніше з точки зору пам'яті для задач з великою кількістю класів.\n",
        "\n",
        " - Фокальна втрата (Focal Loss): Варіант перехресної ентропії, який призначений для зосередження більшої уваги на важких або погано класифікованих прикладах, і часто використовується для задач об'єктного виявлення, де існує дисбаланс класів.\n",
        "\n",
        "Вибір функції втрат залежить від конкретної задачі, розподілу даних та бажаної поведінки моделі під час навчання."
      ],
      "metadata": {
        "id": "jIZAjC8_QvQK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Обираючи архітектуру нейронної мережі для задачі класифікації, важливо почати з базової моделі і працювати з ітераціями. Якщо задача не дуже складна і даних небагато, може підійти проста повнозв'язна мережа. Для зображень часто використовуються конволюційні нейронні мережі, що ефективно розпізнають візуальні шаблони. При роботі з послідовностями, такими як текст або часові ряди, зазвичай краще підходять рекурентні нейронні мережі або LSTM. Великі та складні датасети можуть вимагати глибших архітектур з більшою кількістю шарів. Експериментування з різними конфігураціями та налаштуваннями, включаючи розмір шарів, кількість шарів, функції активації та методи регуляризації, є ключем до знаходження найкращої архітектури для конкретної задачі."
      ],
      "metadata": {
        "id": "7QLG5qNLQvSi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.  \n",
        " - Підготовка даних: Збір та очищення даних, розділення на тренувальний, валідаційний та тестовий набори.\n",
        " - Вибір архітектури: Визначення структури мережі, кількості шарів, нейронів у кожному шарі, типу активаційної функції.\n",
        " - Ініціалізація ваг: Ваги можуть бути ініціалізовані випадково або за допомогою більш складних методів ініціалізації.\n",
        " - Вибір функції втрат та оптимізатора: Вибір метрик для вимірювання помилок і алгоритмів для коригування ваг.\n",
        " - Навчання: Подача даних через мережу, розрахунок втрат, зворотне поширення помилки для оновлення ваг.\n",
        " - Оцінка та налаштування: Використання валідаційного набору даних для оцінки моделі та налаштування гіперпараметрів.\n",
        " - Тестування: Оцінка кінцевої моделі на тестовому наборі даних для перевірки її здатності до узагальнення.\n",
        "\n",
        "Ці кроки можуть включати ітерації та тонке налаштування, залежно від результатів, отриманих під час валідації та тестування."
      ],
      "metadata": {
        "id": "GbyLWqFRQvUm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Keras є високорівневою нейронною мережевою бібліотекою, яка працює як інтерфейс для бібліотеки TensorFlow. Вона дозволяє швидко та зручно створювати різноманітні архітектури нейронних мереж з підтримкою майже усіх стандартних типів мереж. За допомогою Keras можна легко конструювати тренувальні моделі, використовуючи послідовний (Sequential) або функціональний API, де користувач може стекувати шари або створювати комплексні архітектури. Також, Keras надає зручні засоби для компіляції моделі, вказуючи оптимізатор, функцію втрат та метрики оцінки, після чого можна легко тренувати модель на даних, використовуючи метод fit. Keras також підтримує зворотні виклики (callbacks) для моніторингу тренування, ранньої зупинки (early stopping), збереження кращих моделей та інше, що робить процес тренування контрольованим та ефективним."
      ],
      "metadata": {
        "id": "ExEwRUZKQvW8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.\n",
        "- Accuracy: Відсоток правильно класифікованих випадків з усіх проб.\n",
        "- Precision: Частка правильно ідентифікованих позитивних результатів серед усіх позитивних результатів, які передбачила модель.\n",
        "- Recall: Частка правильно ідентифікованих позитивних результатів серед усіх фактичних позитивних випадків.\n",
        "- F1-score: Гармонійне середнє точності та повноти, корисне для оцінки балансу між цими двома метриками.\n",
        "- ROC-AUC: Площа під кривою оператора отримання характеристики, що оцінює здатність моделі розрізняти класи.\n",
        "- Матриця помилок: Таблиця, що показує плутанину між фактичними та передбачуваними класами.\n",
        "- Валідація: Використання невидимих даних (валідаційного набору) для перевірки моделі під час тренування.\n",
        "- Крос-валідація: Тренування моделі на різних підмножинах даних та валідація на відповідних комплементарних підмножинах для забезпечення стійкості моделі.\n",
        "- Лосс: Оцінка функції втрат на тестових даних може дати зрозуміння загальної ефективності."
      ],
      "metadata": {
        "id": "IXMevUNWQvZB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. - Перенавчання (Overfitting): коли мережа добре працює на тренувальних даних, але погано на нових даних. Це можна вирішити за допомогою методів регуляризації, таких як Dropout, або застосуванням технік збільшення даних (data augmentation).\n",
        "\n",
        " - Недонавчання (Underfitting): коли мережа не в змозі захопити закономірності в даних, що часто відбувається через надто просту модель або недостатнє тренування. Рішення може полягати у збільшенні складності моделі або тривалішому тренуванні.\n",
        "\n",
        " - Зникнення або вибух градієнтів (Vanishing/Exploding Gradients): коли градієнти стають дуже малими або великими, що ускладнює оновлення ваг. Використання нормалізації шарів, на кшталт Batch Normalization, та ініціалізація ваг Хе або Глорота може допомогти.\n",
        "\n",
        " - Невідповідність розмірностей (Dimension Mismatch): помилки у вхідних або вихідних розмірностях даних. Перевірка та коректування розмірностей на кожному етапі може бути рішенням. Моя найбільша проблема)\n",
        "\n",
        " - Нестача даних: якщо даних недостатньо для тренування, можна використовувати збільшення даних або передатреновані моделі (transfer learning).\n",
        "\n",
        " - Складність обчислень: нейронні мережі можуть вимагати значних обчислювальних ресурсів, особливо під час тренування глибоких мереж. Використання GPU або розподілених систем може допомогти.\n",
        "\n",
        " - Підбір гіперпараметрів: неправильно вибрані гіперпараметри можуть ускладнити тренування. Автоматичний підбір гіперпараметрів за допомогою таких методів, як сітчастий пошук (grid search) або випадковий пошук (random search), може бути корисним.\n",
        "\n",
        " - Дисбаланс класів: якщо деякі класи представлені набагато менше, ніж інші, модель може виявитись упередженою. Техніки збалансування даних, як-от ваги класів або синтетичне генерування даних (SMOTE), можуть допомогти."
      ],
      "metadata": {
        "id": "JWQvi4RaQva2"
      }
    }
  ]
}